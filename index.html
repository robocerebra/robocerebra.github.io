<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <!-- ==================== SEO & Social Preview ==================== -->
  <meta name="description" content="RoboCerebra is a large-scale benchmark that shifts robotic manipulation from reactive System 1 policies toward deliberative System 2 reasoning with long-horizon tasks, a hierarchical VLM-VLA framework and multi-dimensional evaluation metrics."/>
  <meta property="og:title" content="RoboCerebra: A Large-scale Benchmark for Long-Horizon Robotic Manipulation Evaluation"/>
  <meta property="og:description" content="A benchmark and dataset for assessing planning, reflection and memory in vision-language robotic agents performing thousands-step household tasks."/>
  <meta property="og:url" content="https://YOUR_URL_HERE"/>
  <meta property="og:image" content="static/images/og_banner.png"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="RoboCerebra Benchmark"/>
  <meta name="twitter:description" content="Long-horizon robotic manipulation benchmark with System 2 reasoning."/>
  <meta name="twitter:image" content="static/images/twitter_banner.png"/>
  <meta name="twitter:card" content="summary_large_image"/>

  <meta name="keywords" content="robotics, benchmark, long-horizon, vision-language, system 2, manipulation"/>
  <meta name="viewport" content="width=device-width, initial-scale=1"/>

  <title>RoboCerebra: Long-Horizon Robotic Manipulation Benchmark</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"/>

  <!-- Google fonts & Bulma -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro&display=swap" rel="stylesheet"/>
  <link rel="stylesheet" href="static/css/bulma.min.css"/>
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css"/>
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"/>
  <link rel="stylesheet" href="static/css/index.css"/>

  <!-- JS (placed in head for Bulma-carousel init) -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="static/js/bulma-carousel.min.js" defer></script>
  <script src="static/js/index.js" defer></script>
</head>
<body>
  <!-- ==================== HERO ==================== -->
  <section class="hero is-light is-fullwidth">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h1 class="title is-1">RoboCerebraï¼šA Large-scale Benchmark for Long-Horizon Robotic Manipulation Evaluation<br></h1>

        <!-- ======== AUTHORS ======== -->
        <div class="is-size-5 publication-authors mb-2">
          Songhao Han<sup>1*</sup>,
          Boxiang Qiu<sup>1*</sup>,
          Yue Liao<sup>2*â€ </sup>,
          Siyuan Huang<sup>3</sup>,
          Chen Gao<sup>1</sup>,
          Shuicheng Yan<sup>2</sup>,
          Si Liu<sup>1â€¡</sup>
        </div>
        <div class="is-size-5 publication-authors">
          <sup>1</sup>Beihang University,&nbsp;
          <sup>2</sup>National University of Singapore,&nbsp;
          <sup>3</sup>Shanghai Jiao Tong University
        </div>
        <div class="is-size-7 publication-authors mt-1">
          <sup>*</sup>Equal contribution&nbsp;â€¢&nbsp;
          <sup>â€ </sup>Project Leader&nbsp;â€¢&nbsp;
          <sup>â€¡</sup>Corresponding author
        </div>


        <!-- ======== LINKS ======== -->
        <div class="buttons is-centered mt-4">
          <!-- arXiv link with Academicons icon -->
          <a href="https://www.arxiv.org/abs/2506.06677" target="_blank" class="button is-dark is-rounded">
            <span class="icon">
              <i class="ai ai-arxiv" style="font-size:1.2rem;"></i>
            </span>
            <span>arXiv</span>
          </a>
          <!-- Hugging Face dataset link with emoji icon -->
          <a href="https://huggingface.co/datasets/qiukingballball/RoboCerebraBench" target="_blank" class="button is-dark is-rounded">
            <span class="icon" style="font-size:1.2rem;">ðŸ¤—</span>
            <span>Dataset</span>
          </a>
          <!-- GitHub code link with icon -->
          <a href="https://github.com/qiuboxiang/RoboCerebra" target="_blank" class="button is-dark is-rounded">
            <span class="icon">
              <img src="static/images/github_icon.png" alt="GitHub" style="width:20px; height:20px; filter: brightness(0) invert(1);">
            </span>
            <span>Code</span>
          </a>
        </div>
      </div>
    </div>
  </section>

<!-- ==================== Video Examples ==================== -->
  <section class="section" id="examples">
    <div class="container is-max-desktop">
      <h2 class="title is-4 has-text-centered">Examples</h2>

      <!-- Ideal Tasks -->
      <figure class="mb-6">
        <figcaption class="has-text-centered is-size-5 mb-4">
          <strong>Ideal Tasks</strong>
        </figcaption>
        <div class="columns is-multiline is-variable is-2">
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/ideal1.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/ideal2.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/ideal3.mp4" controls style="width:100%; height:auto;"></video>
          </div>
        </div>
      </figure>

      <!-- Memory Exploration -->
      <figure class="mb-6">
        <figcaption class="has-text-centered is-size-5 mb-4">
          <strong>Memory Exploration (Exp.)</strong>
        </figcaption>
        <div class="columns is-multiline is-variable is-2">
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Memory_Exploration1.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Memory_Exploration2.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Memory_Exploration3.mp4" controls style="width:100%; height:auto;"></video>
          </div>
        </div>
      </figure>

      <!-- Memory Execution -->
      <figure class="mb-6">
        <figcaption class="has-text-centered is-size-5 mb-4">
          <strong>Memory Execution (Exe.)</strong>
        </figcaption>
        <div class="columns is-multiline is-variable is-2">
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Memory_Execution1.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Memory_Execution2.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Memory_Execution3.mp4" controls style="width:100%; height:auto;"></video>
          </div>
        </div>
      </figure>

      <!-- Random Disturbance -->
      <figure class="mb-6">
        <figcaption class="has-text-centered is-size-5 mb-4">
          <strong>Random Disturbance (Ran.)</strong>
        </figcaption>
        <div class="columns is-multiline is-variable is-2">
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/dynamic1.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/dynamic2.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/dynamic3.mp4" controls style="width:100%; height:auto;"></video>
          </div>
        </div>
      </figure>

      <!-- Mix -->
      <figure class="mb-6">
        <figcaption class="has-text-centered is-size-5 mb-4">
          <strong>Mix</strong>
        </figcaption>
        <div class="columns is-multiline is-variable is-2">
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Mix1.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Mix2.mp4" controls style="width:100%; height:auto;"></video>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/Mix3.mp4" controls style="width:100%; height:auto;"></video>
          </div>
        </div>
      </figure>

    </div>
  </section>




  <!-- ==================== FIGURE ==================== -->
  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <figure>
        <img src="static/images/fig1.png" style="width:100%; height:auto;" alt="RoboCerebra pipeline overview"/>
        <figcaption class="has-text-grey mt-2">Overview of the <strong>RoboCerebra</strong> data-generation pipeline and long-horizon task examples.</figcaption>
      </figure>
    </div>
  </section>

  <!-- ==================== ABSTRACT ==================== -->
  <section class="section has-background-light" id="abstract">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Abstract</h2>
      <div class="content has-text-justified">
        <p>Recent advances in vision-language models (VLMs) have enabled instruction-conditioned robotic systems with improved generalization. However, most existing work focuses on reactive <em>System 1</em> policies, underutilizing VLMsâ€™ strengths in semantic reasoning and long-horizon planning. These System 2 capabilitiesâ€”characterized by deliberative, goal-directed thinkingâ€”remain underexplored due to the limited temporal scale and structural complexity of current benchmarks. To address this gap, we introduce <strong>RoboCerebra</strong>, a benchmark for evaluating high-level reasoning in long-horizon robotic manipulation. RoboCerebra includes: (1) a large-scale simulation dataset with extended task horizons and diverse subtask sequences in household environments; (2) a hierarchical framework combining a high-level VLM planner with a low-level vision-language-action (VLA) controller; and (3) an evaluation protocol targeting planning, reflection, and memory through structured System 1â€“System 2 interaction. The dataset is constructed via a top-down pipeline, where GPT generates task instructions and decomposes them into subtask sequences. Human operators execute the subtasks in simulation, yielding high-quality trajectories with dynamic object variations. Compared to prior benchmarks, RoboCerebra features significantly longer action sequences and denser subtask annotations, forming a more rigorous testbed for evaluating System 2 reasoning.</p>
      </div>
    </div>
  </section>

  <!-- ==================== DATASET ==================== -->
  <section class="section" id="dataset">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">RoboCerebra Dataset</h2>
      <div class="content has-text-justified">
        <p><strong>RoboCerebra</strong> contains <strong>1,000</strong> human-annotated trajectories across <strong>100</strong> task variants, each spanning up to 3,000 simulation steps. Tasks cover everyday household activities (e.g., preparing drinks, tidying groceries) and are annotated with fine-grained subtask boundaries, temporal segments, and dynamic scene variations.</p>
        <ul>
          <li><strong>Extended Horizons :</strong> average trajectory length 2,972 steps â€” about 6Ã— longer than prior datasets.</li>
          <li><strong>Compositional Diversity :</strong> 12 primitive action categories; >10 % of tasks require 5+ action types.</li>
          <li><strong>Dynamic & Memory-Dependent :</strong> random disturbances, partial observability, and memory-centric exploration/execution modes.</li>
        </ul>
      </div>
      <figure class="has-text-centered">
        <img src="static/images/fig3.png" style="width:100%; height:auto;" alt="Dataset statistics"/>
      </figure>
    </div>
  </section>
  <!-- ==================== FRAMEWORK ==================== -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Hierarchical Planning & Execution Framework</h2>
  
      <!-- Success & Failure Examples -->
      <figure class="mb-6">

        <!-- Success æ ‡é¢˜ -->
        <figcaption class="has-text-centered is-size-5 mb-2">
          <strong>Success Examples</strong>
        </figcaption>
        <div class="columns is-multiline is-variable is-2">
          <!-- success -->
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/place_frypan_on_wooden_cabinet_top_side_success.mp4" controls style="width:100%; height:auto;"></video>
            <figcaption class="mt-2">place frypan on wooden cabinet top side</figcaption>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/place_the_plate_on_dining_set_group_success.mp4" controls style="width:100%; height:auto;"></video>
            <figcaption class="mt-2">place the plate on dining set group</figcaption>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/pour_out_wine_from_the_wine_bottle_onto_the_plate_success.mp4" controls style="width:100%; height:auto;"></video>
            <figcaption class="mt-2">pour out wine from the wine bottle onto the plate</figcaption>
          </div>
        </div>

        <!-- Failure æ ‡é¢˜ -->
        <figcaption class="has-text-centered is-size-5 mb-2">
          <strong>Failure Examples</strong>
        </figcaption>
        <div class="columns is-multiline is-variable is-2">
          <!-- fail -->
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/open_the_short_cabinet_in_middle_region_fail.mp4" controls style="width:100%; height:auto;"></video>
            <figcaption class="mt-2">open the short cabinet in middle region</figcaption>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/pick_up_the_chocolate_pudding_from_wooden_cabinet_fail.mp4" controls style="width:100%; height:auto;"></video>
            <figcaption class="mt-2">pick up the chocolate pudding from wooden cabinet</figcaption>
          </div>
          <div class="column is-one-third-desktop is-full-mobile has-text-centered">
            <video src="static/videos/place_down_the_cream_cheese_into_short_cabinet_mid_fail.mp4" controls style="width:100%; height:auto;"></video>
            <figcaption class="mt-2">place down the cream cheese into short cabinet middle region</figcaption>
          </div>
        </div>

      </figure>

  
      <div class="content has-text-justified">
        <p>At inference time, the VLM parses a high-level task instruction into a sequence of step-level subgoals, which are stored in a memory bank. The VLA continuously queries the active subgoal and executes corresponding low-level actions based on high-frequency visual observations. Concurrently, the VLM periodically attends to recent observations to monitor execution progress. Upon detecting subgoal completion or deviation, it updates the memory with the next subgoal or issues a refined instruction. This closed-loop coordination preserves temporal abstraction while ensuring reactive control, enabling robust and interpretable performance in long-horizon tasks.</p>
      </div>
      <!-- ä¸‹é¢ä¿æŒåŽŸæœ‰çš„ framework å›¾ -->
      <figure class="has-text-centered">
        <img 
          src="static/images/fig4.png" 
          alt="Hierarchical Planning & Execution Framework" 
          style="width:100%; height:auto;" />
      </figure>
    </div>
  </section>

  <!-- ==================== RESULTS ==================== -->
  <section class="section" id="results">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Benchmark Results</h2>
      <div class="content has-text-justified">
        <p>We evaluate each method over 600 rollouts (60 tasks Ã— 10 trials). For fair comparison across planning models, we define a set of anchor points that determine when System 1 transitions between subgoals. These anchor-aligned transitions decouple step-switching from the model, allowing consistent temporal granularity across models.</p>
      </div>
      <figure class="has-text-centered">
        <img src="static/images/table3.png" style="width:100%; height:auto;" alt="Main quantitative results"/>
      </figure>
    </div>
  </section>

  <!-- ==================== BIBTEX ==================== -->
  <section class="section has-background-light" id="bibtex">
    <div class="container is-max-desktop content">
      <h2 class="title is-3 has-text-centered">BibTeX</h2>
      <pre><code>@article{han2025robocerebra,
  title={RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation},
  author={Han, Songhao and Qiu, Boxiang and Liao, Yue and Huang, Siyuan and Gao, Chen and Yan, Shuicheng and Liu, Si},
  journal={arXiv preprint arXiv:2506.06677},
  year={2025}
}</code></pre>
    </div>
  </section>

  <footer class="footer has-text-centered">
    <div class="content">
      <p>Website adapted from the <a href="https://github.com/Diffusion-CoT/reflection2perfection" target="_blank">Academic Project Page Template</a>.</p>
    </div>
  </footer>
</body>
</html>
